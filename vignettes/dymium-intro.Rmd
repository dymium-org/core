---
title: "Introduction to dymiumCore"
author: "Amarin Siripanich"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true

vignette: >
  %\VignetteIndexEntry{Introduction}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(dymiumCore)
```

This vignette introduces the basic building blocks for microsimulation in `dymiumCore`, how they can be put together to create a microsimulation model, and how to take advantage of existing modules from the [dymiumModules](https://github.com/dymium-org/dymiumModules) repository. 

# Why another microsimulation package?

As an R user, I feel that there are so many great packages that can makes R a complete platform for microsimulation and yet, to my knowledge, no one has taken advantage of this fact. The stages of microsimulation modelling is no different from the stages of any other modelling excerises in general, at least at the high-level. They can be broken down as the following:

1. __Data preparation__: synthesise microdata and estimate models. 
2. __Microsimulation__: microsimulate agents in the microdata using the estimated models.
3. __Validation__: check the simulation result againts real-world observed statistics. 
4. __Calibration__: slightly nudge some parameters of the estimated models to have a better validation result.
5. __Sensitivity analysis__: make use of the model for policy analysis.

These stages often done under different programming environments which makes it extremely hard for anyone to keep track of the scripts, the package dependencies, the software licences involved, and the data produced during the development phrase. Inevitably, those issues will cause the model to be shorted-lived. These issues can be solved effectively with packages that are readily avaiable in R.

## Data preparation

In the first stage, packages for data manipulation such as [data.table](http://r-datatable.com), [dplyr](https://dplyr.tidyverse.org), the recent dplyr-like packages that use data.table as their backend such as [dtplyr](https://github.com/tidyverse/dtplyr) and [tidydt](https://github.com/markfairbanks/tidydt), and [sf](https://r-spatial.github.io/sf/index.html) for working with geopgrahical data are immensely useful. 

### Systhesise microdata

To represent the initial state of the simulation we need microdata and often the microdata that we need is only available as a small sample of the true population. This is to prevent the survey participants from being identified. Hence, we often need to synthesise/reconstruct a population that resembles the true population using the microdata and some known margins of the true population. Luckily, as an R user you have many well deleloped packages to choose from for systhesising data such as [MultiLevelIPF](https://github.com/krlmlr/MultiLevelIPF), [simPop](https://cran.r-project.org/web/packages/simPop/index.html), [mipfp](https://cran.r-project.org/web/packages/mipfp/mipfp.pdf). Even a book on this topic, [Spatial Microsimulation with R by Robin Lovelace and Morgane Dumont](https://spatial-microsim-book.robinlovelace.net), in R is available!

### Esimate models

Statistical modelling is one of the main strengths of the R ecosystem. My very impression of R was how easy it was for me to fit various models from a OLS regression to a binary logit modelling using just one function, which is `stats::glm()`. Since, most microsimulation models relies on probalistic models, (and also rule-based models) to simulate the decisions made by its agents, there are a vast amount of packages that can facilitate model fitting. `stats`, [caret](https://cran.r-project.org/web/packages/caret/caret.pdf), [mlr](https://mlr.mlr-org.com), [mlogit](https://cran.r-project.org/web/packages/mlogit/index.html) and more can be used for this purpose. Even if a person only know the `caret` package alone, he/she can have access to 238 different model implementations, from basic econometric models to advanced machine-learning models. This provides an extensive playground for microsimulation modellers be more experimental and go beyond the traditional knowledge of the field. 

## Microsimulation

There are very few R packages for microsimulation. The few packages that are available are somewhat feature lacking and not meant for a large-scale microsimulation model that is flexible to addition of new behavior and easy to maintain. Hence, this is where we, the authors, see a place for `dymiumCore` in the R package ecosystem. 

## Validation

A microsimulation model should be validated visually and numerically. For visualisation, most R users probably turn to [ggplot2](https://ggplot2.tidyverse.org) and its [extensions](http://www.ggplot2-exts.org/gallery/). Spatial data can be visualised using `ggmap`, `leaflet`, `mapview` and more. For numerical comparison, [Metrics](https://cran.r-project.org/web/packages/Metrics/index.html) provides many commonly used evaluation metrics for regression and classification problems. 

## Calibration

Calibration is often used in large-scale models (a system of many models), which many microsimulation models are, to adjust for their errors and ultimately improve the accuracy of the submodels and the overall result. As you may know, calibration is a complex process especially for large-scale models, which are often viewed as black-box models, as it can be very expensive (time and computing resources) to perform. However, if you wish to calibrate your microsimulation model [mlrMBO](https://mlrmbo.mlr-org.com) are here for you.

_(A tutorial on using `mlrMBO` for calibration of a dymium microsimulation model is being drafted and will be released soon.)_

## Reproducibility

More general issues such as reproducibility and maintainability of the programming environments used can be easily solved using [drake](https://github.com/ropensci/drake) and [renv](https://github.com/rstudio/renv). `drake` helps you to manage your workflow in an organised and reproducible way while `renv` takes care of your dependencies.

# Main requirements

dymiumCore relies on a number of R packages under the hood such as [R6](https://r6.r-lib.org) and [data.table](http://r-datatable.com). Hence, if you are looking to extend dymiumCore or develop new modules that are compatible a good understanding of those mentioned packages is the one of main requirements. 

# Overview of the building blocks 

Microsimulation models, regardless of their study contexts, generally share the same basic components which are entities and rules. The rest of the article focuses on the main building blocks of microsimulation in term of implementation. 

## Entities

Entities can be persons, firms, buildings, zones, transport network etc. They
can be conviniently defined as a class, based on the concept of 'objects' in 
object-oriented programming. Each entity is known by its fields and methods, 
what it represents and what it can do. A person may contains fields such as age, 
gender, marital status. A person give birth, leave parental home, etc.  While 
a household may have household size, household id, number of vehicles owned as
its fields. A household can relocate, have new members and etc.

## Transition

Entities are usually given rules for them to follow 


In a microsimulation model, entities are given rules for them to follow under 
different conditions. A rule can simply be an ifelse statement such as:

```
if age is greater than 16:
    can_marry 
else:
    cant_marry
```

or probabilistic such as a rate-based model or a classification model 
(binary logit model, multinomial logit model, hazard-based model, random forest, 
and artificial neural network) that takes attributes of the entities as the
input variables.

dymium provides a class called `TransitionClassification` and `TransitionRegression` which take in a rule and entities then simulate the outcomes of the entities given the provided rule.  For probabilistic rules, Monte Carlo simulation will be performed based on the probabilistic values from the rules.

Currently, `dymiumCore` only supports the model objects fitted using `caret` and `stats` in the `Transition` classes.

| Package |        Class |                                     Model types |         status |
| ------: | -----------: | ----------------------------------------------: | -------------: |
|   caret |      `train` |            classification and regression models |      supported |
|   stats | `lm` & `glm` |                   generalised regression models |      supported |
|  mlogit |     `mlogit` |                        multinomial logit models | in-development |
|    mlr3 |    `Learner` | classification, survival, and regression models |        planned |

In microsimulation, modellers often use transition probabilities that they obtained 
from official sources. Hence, __dymiumCore__ provides an easy step to use those
transition probabilities in `Transition`. __A static rate model__, __a dynamic rate model__, 
and __a enumerated choice model__ can be provided in a data.table format. When a
data.table is provided as the model argument to `Transition`, `Transition`  will 
figure out which of the three models the provided data.table matches based on the following criteria. 

It is __a static rate model__, if the data.table contains a column called `prob` which is
of type numeric with the value between 0 to 1.

```{r}
library(data.table) # use install.packages("data.table") to install

(rate_based_model <-
  data.table(
  sex = c("male", "female"),
  prob = c(0.3, 0.2)
))
```

It is __a dynamic rate model__, if the provided data.table contains columns that indicate
time periods with a prefix `t_` follow by a numeric value i.e.  `t_0`, `t_2011`, `t_2050`.
Those time period columns must be of type numeric and the values under those columns 
must be within 0 to 1. 

```{r}
(dynamic_rate_based_model <-
  data.table(
  sex = c("male", "female"),
  prob = c(0.3, 0.2),
  t_2010 = c(0.5, 0.2),
  t_2011 = c(0.3, 0.1),
  t_2012 = c(0.4, 0.3)
))
```

It is __a choice model__, of the provided data.table contains columns that called
`probs` and `choices` of the type list that contains numeric vectors and character
vectors, respectively.  

```{r}
(choice_model <-
  data.table(
    sex = c('male', 'female'),
    probs = list(c(0.3,0.7), c(0.4,0.6)),
    choices = list(c('can drive', 'cannot drive'), c('can drive', 'cannot drive'))
))
```

__Note that__, if more the provided data.table matches more than one of the three formats
then `Transition` will raise an error. 

__Suggestion:__  you can import transition probability tables that are in a `csv` format
or a `.xlsx` format to R using the `fread` function from the `data.table` package or `read.csv()`
from the `utils` package that comes preinstalled with R or `read_xlsx()` from the
`readxl` package for xlsx files.

- A named `list` can be used to represent choices, where the names of the list are 
choices and their values are their associated probabilities. 

```{r}
(binary_list_model <- list(yes = 0.05, no = 0.95))
(marriage_model <- list(married = 0.05, not_married = 0.95))
(employment_model <- list(employed = 0.4, unemployed = 0.2, not_in_labour_force = 0.3))
```

In a binary choice model, it is recommended that the outcome variable should be 
coded as "yes" and "no" as many of the modules available at [dymium-org/dymiumModules](https://github.com/dymium-org/dymiumModules)
use with this convention.

## Market

In microsimulation, a market is where entities are directly interact with one another.
A market can be an abstraction of a real-estate bidding market, a mate matching market,
a labour market, etc. The entire market are separated into two sides (i.e. proposers 
and proprosees, buyers and sellers, labours and firms). Interactions maybe one-sided 
or two-sided depends on the matching mechanism that is used. When it is
one-sided only agents from one side of the market make the decision about what they 
get based on their preference. When it is the matching problem is two-sided one 
then both sides of the market evaluate evaluate options available to them and interact 
in a way that mimic bidding.

Note that, the number of alternatives available to each agent is customisable to realistically 
represent a matching situation that is being simulated. For example, in a housing
search situation, it might be inappropiate to assume that all relocating households 
have perfect knowledge about all the available dwelling units that are on the market
at any moment in time. Hence, the modeller might want to limit the number of alternatives
available in the choiceset of each household or the number of zones that the households
look for their options. 

Currently, there are two implementations of market matching algorithms which are

- Stochastic matching (`MatchingMarketStochastic`) and
- Optimal matching (`MatchingMarketOptimal`)

In `MatchingMarketStochastic`, only one-sided matching problems can be solved. All
agents that are seeking a match are randomly ordered into a virtual queue.
The first agent in the queue gets to select an alternative among all the alternatives
that available to it. While in `MatchingMarketOptimal`, both one-sided and two-sided 
problems can be solved. All agents are aware of all the available alternatives 
(e.g. houses, partners, etc.) in the market. This generally a very strong assumption.
Hence, to mitigate such strong assumption the whole market can be further segmented 
into sub markets where agents that are more alike or geographically near one others
are stratified into the same sub market and only aware of each others and not those
that are their unlikely matches.

## Logging

There are a few ways that you can log in dymium. To choose which of the logging methods is suitable in your please consider the following. Please note that, this subsection is only to give a brief overview of the methods so please see its documentation for more details.

### Logging with `lgr`

The `lgr` package is used by dymiumCore internally. This sometimes give warnings on the console. You can also see lower-level log messages if you change the log threshold.

```{r, eval = FALSE}
dymiumCore:::lg$set_threshold("info")
dymiumCore:::lg$set_threshold("trace")
```

The default threshold for is set to `warn` which only shows warning messages and above (i.e. debug, fatal). As you can see, the internal logger is not exported hence it is not meant to be used by the end-user.

`lgr` is also used in dymium modules. If you create a new module using `use_module()` a lgr logger will be available for you to use within your event functions, see `logger.R` in your module folder.

There are many powerful stuffs you can do with `lgr` and to learn more about it please see its [vignette](https://s-fleck.github.io/lgr/articles/lgr.html). 

### Logging with `Generic$log()`

All classes that inherit `Generic`, namely `Entity` and `Container`, have the `log()` method available to them. The main purpose of this function is to give the user a convinient way to log simulation outputs. `get_log()` can be used on objects that have the `$log()` method to gather all logs.

```{r}
create_toy_world()
total_no_individuals <- world$entities$Individual$n()
world$entities$Individual$log(desc = "total_no_individuals", value = total_no_individuals)
get_log(world)
```

### Logging with `add_history()`

`add_history()` has a more specific used and only applicable to `Entity`, it is meant to be used for logging of past events that occur to each entity. This is particularly useful when past actions of entities can influence their current and future actions. There are helper functions to get and combine histories such as `get_history()` and `combine_histories()`.

```{r}
create_toy_world()
add_history(entity = world$entities$Individual, ids = 1:3, event = "marriage")
get_history(x = world$entities$Individual)
# alternatively you can use insepct to view the attribute data of the individuals
# and their history data at the same time
inspect(entity = world$entities$Individual, ids = 1:3)
```

